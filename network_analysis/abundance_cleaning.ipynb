{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb95c17",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\"> __Data Abundance table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91652fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2bcf0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary Before Deduplication ===\n",
      "Total rows in original DataFrame: 8283371\n",
      "Number of duplicated rows: 574\n",
      "Number of unique groups (by ['loaded_uid', 'ncbi_taxon_id', 'taxon_rank_level']): 8283084\n",
      "\n",
      "=== Summary After Deduplication ===\n",
      "Total rows after deduplication: 8283084\n",
      "Shape of final DataFrame: (8283084, 4)\n",
      "\n",
      "Deduplication complete. Duplicates collapsed by averaging abundance values.\n",
      "\n",
      "=== Taxonomic Rank Distribution ===\n",
      "taxon_rank_level\n",
      "species    5751183\n",
      "genus      2531901\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of Genus-Level entries: 2531901\n",
      "Number of Species-Level entries: 5751183\n",
      "\n",
      "Number of rows with ncbi_taxon_id = -1 (unclassified): 64488\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Load the data ===\n",
    "df = pd.read_csv(\n",
    "    \"/mnt/iusers01/fatpou01/bmh01/msc-bioinf-2024-2025/h44063jg/gm_repository/species_abundance.txt\",\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "# === Step 2: Define ID columns used to identify duplicates ===\n",
    "id_cols = ['loaded_uid', 'ncbi_taxon_id', 'taxon_rank_level']\n",
    "\n",
    "# === Step 3: Identify abundance columns (assumed numeric, exclude ID columns) ===\n",
    "abundance_cols = df.columns.difference(id_cols)\n",
    "\n",
    "# === Step 4: Check for duplicated rows ===\n",
    "duplicate_rows = df[df.duplicated(subset=id_cols, keep=False)]\n",
    "\n",
    "# === Step 5: Summary before deduplication ===\n",
    "print(\"=== Summary Before Deduplication ===\")\n",
    "print(f\"Total rows in original DataFrame: {len(df)}\")\n",
    "print(f\"Number of duplicated rows: {len(duplicate_rows)}\")\n",
    "print(f\"Number of unique groups (by {id_cols}): {df[id_cols].drop_duplicates().shape[0]}\")\n",
    "print()\n",
    "\n",
    "# === Step 6: Deduplicate by averaging relative abundance values ===\n",
    "df = df.groupby(id_cols, as_index=False)[abundance_cols].mean()\n",
    "\n",
    "# === Step 7: Summary after deduplication ===\n",
    "print(\"=== Summary After Deduplication ===\")\n",
    "print(f\"Total rows after deduplication: {len(df)}\")\n",
    "print(f\"Shape of final DataFrame: {df.shape}\")\n",
    "print(\"\\nDeduplication complete. Duplicates collapsed by averaging abundance values.\\n\")\n",
    "\n",
    "# === Step 8: Taxonomic Rank Summary ===\n",
    "print(\"=== Taxonomic Rank Distribution ===\")\n",
    "rank_counts = df['taxon_rank_level'].value_counts()\n",
    "print(rank_counts)\n",
    "\n",
    "# Focused summary for genus and species\n",
    "n_genus = rank_counts.get('genus', 0)\n",
    "n_species = rank_counts.get('species', 0)\n",
    "\n",
    "print(f\"\\nNumber of Genus-Level entries: {n_genus}\")\n",
    "print(f\"Number of Species-Level entries: {n_species}\")\n",
    "\n",
    "# === Step 9: Rows with ncbi_taxon_id == -1 ===\n",
    "n_unclassified = df[df['ncbi_taxon_id'] == -1].shape[0]\n",
    "print(f\"\\nNumber of rows with ncbi_taxon_id = -1 (unclassified): {n_unclassified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2740f425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "loaded_uid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ncbi_taxon_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "taxon_rank_level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "relative_abundance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d00298ad-e67e-43d1-9db0-be0b2c6cbc81",
       "rows": [],
       "shape": {
        "columns": 4,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loaded_uid</th>\n",
       "      <th>ncbi_taxon_id</th>\n",
       "      <th>taxon_rank_level</th>\n",
       "      <th>relative_abundance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [loaded_uid, ncbi_taxon_id, taxon_rank_level, relative_abundance]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows = df[df.duplicated(subset=id_cols, keep=False)]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662a86f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562d9ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "RESULTS — df2 Sample Loading Info (uid → accession_id)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shape: (52633, 8)\n",
      "Nulls (uid/accession_id): {'uid': 0, 'accession_id': 0}\n",
      "Exact (uid, accession_id) duplicate rows: 0\n",
      "uids mapping to >1 accession_id: 0\n",
      "accession_id mapping to >1 uid: 0\n",
      "Unique (run_id, disease) pairs: 59051\n",
      "\n",
      "====================================================================================================\n",
      "RESULTS — df3 Sample to disease Info (run_id → disease)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Duplicated pairs (count > 1): 98\n",
      "Total unique run_ids in pairs (after removing the pairs - deduplicated): 51934\n",
      "Single-disease run_ids:        48349\n",
      "Multi-disease (comorbid):      3585\n",
      "\n",
      "====================================================================================================\n",
      "=== Multi-disease run_ids breakdown ===\n",
      "Total multi-disease (run_ids >1 disease): 3585\n",
      "  exactly 2 diseases:                     1857\n",
      "  >2 diseases:                            1728\n",
      "\n",
      "====================================================================================================\n",
      "=== Counts by number of diseases per run_id ===\n",
      "disease\n",
      "1     48349\n",
      "2      1857\n",
      "3       826\n",
      "4       428\n",
      "5       236\n",
      "6       122\n",
      "7        67\n",
      "8        28\n",
      "9        17\n",
      "10        4\n",
      "Name: count, dtype: int64\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTICS ON MAPPING TABLES\n",
    "# === Paths ===\n",
    "PATH_DF2 = \"/mnt/iusers01/fatpou01/bmh01/msc-bioinf-2024-2025/h44063jg/gm_repository/samples_loaded.txt\"\n",
    "PATH_DF3 = \"/mnt/iusers01/fatpou01/bmh01/msc-bioinf-2024-2025/h44063jg/gm_repository/sample_to_disease_info.txt\"\n",
    "\n",
    "# === Load mapping tables ===\n",
    "df2 = pd.read_csv(PATH_DF2, sep=\"\\t\")  # expects: uid, accession_id\n",
    "df3 = pd.read_csv(PATH_DF3, sep=\"\\t\")  # expects: run_id, disease\n",
    "\n",
    "# ---------- Compute (df2) ----------\n",
    "req2 = {\"uid\", \"accession_id\"}\n",
    "miss2 = req2 - set(df2.columns)\n",
    "if miss2: raise KeyError(f\"df2 missing columns: {miss2}\")\n",
    "\n",
    "df2_shape = df2.shape\n",
    "df2_nulls = df2[[\"uid\",\"accession_id\"]].isna().sum().to_dict()\n",
    "df2_pair_dup_rows = int(df2.duplicated([\"uid\",\"accession_id\"], keep=False).sum())\n",
    "df2_uid_conflicts = int((df2.groupby(\"uid\")[\"accession_id\"].nunique() > 1).sum())\n",
    "df2_acc_conflicts = int((df2.groupby(\"accession_id\")[\"uid\"].nunique() > 1).sum())\n",
    "\n",
    "# ---------- RESULTS ----------\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RESULTS — df2 Sample Loading Info (uid → accession_id)\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Shape: {df2_shape}\")\n",
    "print(f\"Nulls (uid/accession_id): {df2_nulls}\")\n",
    "print(f\"Exact (uid, accession_id) duplicate rows: {df2_pair_dup_rows}\")\n",
    "print(f\"uids mapping to >1 accession_id: {df2_uid_conflicts}\")\n",
    "print(f\"accession_id mapping to >1 uid: {df2_acc_conflicts}\")\n",
    "\n",
    "# ---------- Compute (df3) ----------\n",
    "# 1) Unique (run_id, disease) pairs (exclude no-disease)\n",
    "pairs = (df3[['run_id', 'disease']]\n",
    "         .dropna(subset=['disease'])\n",
    "         .drop_duplicates())\n",
    "\n",
    "print(\"Unique (run_id, disease) pairs:\", len(pairs))\n",
    "\n",
    "# 2) Which pairs were duplicated in the raw data (optional)\n",
    "dup_pairs = (df3[['run_id', 'disease']]\n",
    "             .dropna(subset=['disease'])\n",
    "             .value_counts()\n",
    "             .reset_index(name='count')\n",
    "             .query('count > 1')\n",
    "             .sort_values('count', ascending=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RESULTS — df3 Sample to disease Info (run_id → disease)\")\n",
    "print(\"-\"*100)\n",
    "print(\"Duplicated pairs (count > 1):\", len(dup_pairs))\n",
    "\n",
    "# Assumes `pairs` exists and has columns: run_id, disease\n",
    "# (unique pairs with no missing disease)\n",
    "\n",
    "# Count diseases per run_id\n",
    "counts = pairs.groupby('run_id')['disease'].nunique().rename('n_diseases')\n",
    "\n",
    "# Identify groups\n",
    "multi_run_ids  = counts[counts > 1].index            # comorbid run_ids\n",
    "single_run_ids = counts[counts == 1].index           # single-disease run_ids\n",
    "\n",
    "# Exploded rows for comorbid run_ids (ready for merging)\n",
    "pairs_multi  = pairs[pairs['run_id'].isin(multi_run_ids)].copy()\n",
    "\n",
    "# One row per single-disease run_id (keep the only disease)\n",
    "pairs_single = (pairs[pairs['run_id'].isin(single_run_ids)]\n",
    "                .drop_duplicates('run_id')\n",
    "                .copy())\n",
    "\n",
    "# Optional: readable summary of comorbids\n",
    "multi_summary = (\n",
    "    pairs_multi.groupby('run_id')\n",
    "               .agg(n_diseases=('disease','nunique'),\n",
    "                    diseases=('disease', lambda s: ', '.join(sorted(s.unique()))))\n",
    "               .sort_values('n_diseases', ascending=False)\n",
    ")\n",
    "\n",
    "print(f\"Total unique run_ids in pairs (after removing the pairs - deduplicated): {counts.shape[0]}\")\n",
    "print(f\"Single-disease run_ids:        {len(single_run_ids)}\")\n",
    "print(f\"Multi-disease (comorbid):      {len(multi_run_ids)}\")\n",
    "\n",
    "\n",
    "# Assumes `pairs` has columns: run_id, disease (unique pairs; no NaNs)\n",
    "\n",
    "# Count unique diseases per run_id\n",
    "n_dis = pairs.groupby('run_id')['disease'].nunique()\n",
    "\n",
    "# Focus on multi-disease only\n",
    "multi_mask = n_dis > 1\n",
    "multi_counts = n_dis[multi_mask]\n",
    "\n",
    "# Totals\n",
    "n_multi_total = int(multi_counts.shape[0])\n",
    "n_exact2      = int((multi_counts == 2).sum())\n",
    "n_gt2         = int((multi_counts > 2).sum())\n",
    "\n",
    "# Full breakdown (e.g., 2→#, 3→#, 4→#, ...)\n",
    "breakdown = multi_counts.value_counts().sort_index()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"=== Multi-disease run_ids breakdown ===\")\n",
    "print(f\"Total multi-disease (run_ids >1 disease): {n_multi_total}\")\n",
    "print(f\"  exactly 2 diseases:                     {n_exact2}\")\n",
    "print(f\"  >2 diseases:                            {n_gt2}\")\n",
    "# Summary: counts by number of diseases\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"=== Counts by number of diseases per run_id ===\")\n",
    "print(n_dis.value_counts().sort_index())\n",
    "print(\"=\"*100)\n",
    "\n",
    "# pairs: columns ['run_id', 'disease'], unique pairs, no NaNs\n",
    "# Ensure strings (optional but safe)\n",
    "pairs_use = pairs.assign(\n",
    "    run_id=pairs['run_id'].astype(str),\n",
    "    disease=pairs['disease'].astype(str)\n",
    ")\n",
    "\n",
    "# Dict: run_id -> [disease1, disease2, ...]\n",
    "run_to_diseases = (\n",
    "    pairs_use.groupby('run_id')['disease']\n",
    "             .apply(lambda s: sorted(set(s)))\n",
    "             .to_dict()\n",
    ")\n",
    "# save as json dictionary\n",
    "with open(\"run_to_diseases.json\", \"w\") as f:\n",
    "    json.dump(run_to_diseases, f, indent=2)\n",
    "\n",
    "# Merge accession_id into df\n",
    "df = df.merge(\n",
    "    df2[['uid', 'accession_id']],\n",
    "    how='left',\n",
    "    left_on='loaded_uid',\n",
    "    right_on='uid'  # df2 has unique uid\n",
    ")\n",
    "\n",
    "# Drop helper column from df2\n",
    "df.drop(columns='uid', inplace=True)\n",
    "\n",
    "\n",
    "# Assign disease list to df (None/NaN where no mapping)\n",
    "run_to_diseases = {str(k).strip(): v for k, v in run_to_diseases.items()}\n",
    "df['disease_list'] = df['accession_id'].map(run_to_diseases)\n",
    "\n",
    "# Merge, Drop empty rows, explode, rename disease_list to disease and reset index\n",
    "df = (df.dropna(subset=['disease_list'])\n",
    "        .explode('disease_list')\n",
    "        .rename(columns={'disease_list':'disease'})\n",
    "        .drop(columns='accession_id') \n",
    "        .reset_index(drop=True))\n",
    "\n",
    "# split into df_genus and df_species\n",
    "df_genus   = df[df['taxon_rank_level'].astype(str).str.strip().str.lower().eq('genus')].copy()\n",
    "df_species = df[df['taxon_rank_level'].astype(str).str.strip().str.lower().eq('species')].copy()\n",
    "\n",
    "# save to csv\n",
    "df_genus.to_csv(\"abundance_genus_data.txt\", sep=\"\\t\", index=False)\n",
    "df_species.to_csv(\"abundance_species_data.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mam_myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
